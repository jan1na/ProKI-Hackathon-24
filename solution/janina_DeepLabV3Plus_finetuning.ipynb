{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q595_UJ-U7kN"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision albumentations"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive (for data storage)\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "3uL0sJnLU8F0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from albumentations import Compose, HorizontalFlip, VerticalFlip, RandomRotate90, RandomBrightnessContrast, ElasticTransform, GridDistortion, ShiftScaleRotate\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to resize predictions to original size\n",
    "def resize_to_original(pred, original_size):\n",
    "    return nn.functional.interpolate(pred, size=original_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "# Define augmentations using Albumentations\n",
    "augmentations = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomRotate90(p=0.5),\n",
    "    RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "], additional_targets={'mask': 'mask'}, p=1.0)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_dir, label_dir, transform=None, augmentations=None, num_transformations=50):\n",
    "        self.input_dir = input_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.augmentations = augmentations\n",
    "        self.num_transformations = num_transformations\n",
    "        self.image_filenames = sorted(os.listdir(input_dir))\n",
    "        self.label_filenames = sorted(os.listdir(label_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames) * self.num_transformations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_idx = idx // self.num_transformations\n",
    "        input_path = os.path.join(self.input_dir, self.image_filenames[image_idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_filenames[image_idx])\n",
    "\n",
    "        input_image = Image.open(input_path).convert(\"RGB\")\n",
    "        label_image = Image.open(label_path).convert(\"L\")\n",
    "        input_image_np, label_image_np = np.array(input_image), np.array(label_image)\n",
    "        original_size = input_image.size[::-1]  # Reverse to (H, W) format\n",
    "\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=input_image_np, mask=label_image_np)\n",
    "            input_image = Image.fromarray(augmented['image'])\n",
    "            label_image = Image.fromarray(augmented['mask'])\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            label_image = torch.from_numpy(np.array(label_image, dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "        return input_image, label_image, original_size\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "batch_size = 8\n",
    "num_transformations = 10\n",
    "\n",
    "# Define data transforms\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset paths\n",
    "input_dir_train = \"/content/drive/MyDrive/ProKI-Hackathon/train/images/\"\n",
    "label_dir_train = \"/content/drive/MyDrive/ProKI-Hackathon/train/masks/\"\n",
    "\n",
    "input_dir_val = \"/content/drive/MyDrive/ProKI-Hackathon/val/images/\"\n",
    "label_dir_val = \"/content/drive/MyDrive/ProKI-Hackathon/val/masks/\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CustomDataset(\n",
    "    input_dir=input_dir_train,\n",
    "    label_dir=label_dir_train,\n",
    "    transform=data_transforms[\"train\"],\n",
    "    augmentations=augmentations,\n",
    "    num_transformations=num_transformations\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    input_dir=input_dir_val,\n",
    "    label_dir=label_dir_val,\n",
    "    transform=data_transforms[\"val\"],\n",
    "    augmentations=None  # Disable augmentations for validation\n",
    ")\n",
    "\n",
    "# Padding function\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def pad_image(img, target_size):\n",
    "    _, h, w = img.shape\n",
    "    pad_h = target_size[0] - h\n",
    "    pad_w = target_size[1] - w\n",
    "    padding = (0, pad_w, 0, pad_h)  # left, right, top, bottom\n",
    "    return pad(img, padding)\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    images, masks, original_sizes = zip(*batch)\n",
    "    max_height = max(img.shape[1] for img in images)\n",
    "    max_width = max(img.shape[2] for img in images)\n",
    "\n",
    "    padded_images = [pad_image(img, (max_height, max_width)) for img in images]\n",
    "    padded_masks = [pad_image(mask, (max_height, max_width)) for mask in masks]\n",
    "\n",
    "    return torch.stack(padded_images), torch.stack(padded_masks), original_sizes\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# Load and modify the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(256, 1, kernel_size=(1, 1))  # For binary segmentation\n",
    "model.aux_classifier[4] = nn.Conv2d(256, 1, kernel_size=(1, 1))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks, original_sizes in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]\n",
    "\n",
    "        # Initialize batch loss for this iteration\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        for output, mask, original_size in zip(outputs, masks, original_sizes):\n",
    "            # Resize the output to the original size of the mask\n",
    "            resized_output = resize_to_original(output.unsqueeze(0), original_size).squeeze(0)\n",
    "            resized_mask = resize_to_original(mask.unsqueeze(0), original_size).squeeze(0)\n",
    "\n",
    "            # Compute loss for each pair\n",
    "            loss = criterion(resized_output, resized_mask)\n",
    "            batch_loss += loss  # Accumulate as tensor\n",
    "\n",
    "        # Average the loss over the batch (keep it as tensor)\n",
    "        train_loss += batch_loss.item() / len(images)\n",
    "\n",
    "        # Backpropagation\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, original_sizes in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)[\"out\"]\n",
    "\n",
    "            # Initialize batch loss for this iteration\n",
    "            batch_loss = 0.0\n",
    "\n",
    "            for output, mask, original_size in zip(outputs, masks, original_sizes):\n",
    "                # Resize the output to the original size of the mask\n",
    "                resized_output = resize_to_original(output.unsqueeze(0), original_size).squeeze(0)\n",
    "                resized_mask = resize_to_original(mask.unsqueeze(0), original_size).squeeze(0)\n",
    "\n",
    "                # Compute loss for each pair\n",
    "                loss = criterion(resized_output, resized_mask)\n",
    "                batch_loss += loss  # Accumulate as tensor\n",
    "\n",
    "            # Average the loss over the batch (keep it as tensor)\n",
    "            val_loss += batch_loss.item() / len(images)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "save_path = \"/content/drive/MyDrive/ProKI-Hackathon/models/deeplabv3_binary_segmentation_42_i_correct.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}!\")"
   ],
   "metadata": {
    "id": "e6vHerk9U8a2"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
